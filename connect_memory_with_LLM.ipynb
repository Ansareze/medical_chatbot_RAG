{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bc9a0d6",
   "metadata": {},
   "source": [
    "# Steps\n",
    "## 1.Setup LLM (Mistral with Huggingface)\n",
    "## 2.Connect LLM with FAISS\n",
    "## 3.Create Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eca477f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71d17b9",
   "metadata": {},
   "source": [
    "# Step 1 - Setup LLM (Mistral with Huggingface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0caccbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "HUGGINGFACE_REPO_ID=\"microsoft/DialoGPT-medium\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62bcec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm(huggingface_repo_id):\n",
    "    # Load model and tokenizer locally\n",
    "    model_name = \"microsoft/DialoGPT-medium\"  # Using a smaller, supported model\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=512,\n",
    "        temperature=0.5,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    # Create HuggingFacePipeline\n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1639b446",
   "metadata": {},
   "source": [
    "## Step 2 - Connect LLM with FAISS and Create Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca61775f",
   "metadata": {},
   "source": [
    "### Custom prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1c808a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_PROMPT_TEMPLATE = \"\"\"\n",
    "Use the pieces of information provided in the context to answer user's question.\n",
    "If you dont know the answer, just say that you dont know, dont try to make up an answer. \n",
    "Dont provide anything out of the given context\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Start the answer directly. No small talk please.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94d047e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_custom_prompt(custom_prompt_template):\n",
    "    prompt=PromptTemplate(template=custom_prompt_template, input_variables=[\"context\", \"question\"])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2f4308f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ansar\\anaconda3\\envs\\new_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ansar\\anaconda3\\envs\\new_env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Database\n",
    "DB_FAISS_PATH=\"vectorstore/db_faiss\"\n",
    "embedding_model=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "db=FAISS.load_local(DB_FAISS_PATH, embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053870c1",
   "metadata": {},
   "source": [
    "## Create QA chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdcb5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain=RetrievalQA.from_chain_type(\n",
    "    llm=load_llm(HUGGINGFACE_REPO_ID),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_kwargs={'k':3}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={'prompt':set_custom_prompt(CUSTOM_PROMPT_TEMPLATE)}\n",
    ")\n",
    "\n",
    "# Now invoke with a single query\n",
    "user_query=input(\"Write Query Here: \")\n",
    "response=qa_chain.invoke({'query': user_query})\n",
    "print(\"RESULT: \", response[\"result\"])\n",
    "print(\"SOURCE DOCUMENTS: \", response[\"source_documents\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
